{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Topic Modeling with Spacy and Gensim.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/robertdnolan/Machine-Learning/blob/master/NLP_Topic_Modeling_with_Spacy_and_Gensim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5_LYhfb97KR",
        "colab_type": "code",
        "outputId": "00ba68b9-f787-43f8-f1b8-b3d684eb62ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 658
        }
      },
      "source": [
        "import spacy #\n",
        "\n",
        "#https://radimrehurek.com/gensim/tut3.html\n",
        "# from gensim import corpora\n",
        "\n",
        "nlp = spacy.load('en') #en_core_web_sm\n",
        "\n",
        "# Sample comments\n",
        "com1 = nlp(\"the bar chart isn't working\") #nlp(\"We need to fix the bar chart. Data is innaccurate.\")\n",
        "com2 = nlp(\"Really like the report! Would be nice to add drill down.\")\n",
        "com3 = nlp(\"You did a great job!\")\n",
        "\n",
        "# Sample categories\n",
        "cat1 = nlp('fix')   #nlp(\"Bug fix issue problem broke innaccurate wrong poor slow not error\")\n",
        "cat2 = nlp('great') #nlp(\"Great nice perfect awesome good fast excellent love well done like\")\n",
        "cat3 = nlp('add')   #nlp(\"Enhance add new addition future modify update improve augment enrich also\")\n",
        "\n",
        "# Parse out verbs and nouns from the comment\n",
        "def get_keywords(string):\n",
        "  \"\"\"\n",
        "  input:  a string or sentence\n",
        "  output: the key words or phrases as a spacy NLP class\n",
        "  \"\"\"\n",
        "  spacy_string = nlp(string)\n",
        "  output_string = ''\n",
        "  for token in spacy_string: \n",
        "    if token.pos_ in ['VERB', 'ADJ']: #'NOUN', \n",
        "      output_string += str(token) + ' ' #token.lemma_)\n",
        "  output_string = nlp(output_string)     \n",
        "  return(output_string)\n",
        "\n",
        "# Which category is the comment most similar to?\n",
        "mod_com1 = get_keywords(str(com1))\n",
        "print(mod_com1)\n",
        "print('Should be cat1')\n",
        "print(mod_com1.similarity(cat1))\n",
        "print(mod_com1.similarity(cat2))\n",
        "print(mod_com1.similarity(cat3))\n",
        "\n",
        "print(\"\")\n",
        "mod_com2 = get_keywords(str(com2))\n",
        "print(mod_com2)\n",
        "print('Should be cat3')\n",
        "print(mod_com2.similarity(cat1))\n",
        "print(mod_com2.similarity(cat2))\n",
        "print(mod_com2.similarity(cat3))\n",
        "\n",
        "print(\"\")\n",
        "mod_com3 = get_keywords(str(com3))\n",
        "print(mod_com3)\n",
        "print('Should be cat2')\n",
        "print(mod_com3.similarity(cat1))\n",
        "print(mod_com3.similarity(cat2))\n",
        "print(mod_com3.similarity(cat3))\n",
        "\n",
        "noun_chunks = list(com1.noun_chunks)\n",
        "print(noun_chunks)\n",
        "print('')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is working \n",
            "Should be cat1\n",
            "-0.05090581993302723\n",
            "0.02429078948424838\n",
            "-0.07299852033987032\n",
            "\n",
            "Would be nice add \n",
            "Should be cat3\n",
            "0.2899073707270399\n",
            "0.2560098396484779\n",
            "0.4683160251821671\n",
            "\n",
            "did great \n",
            "Should be cat2\n",
            "0.19075471952637063\n",
            "0.4763020021357786\n",
            "0.32295071202846126\n",
            "[the bar chart]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n",
            "/usr/lib/python3.6/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq7Xs1ZeOa7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from gensim import corpora\n",
        "# dictionary = corpora.Dictionary.load('/tmp/deerwester.dict')\n",
        "# corpus = corpora.MmCorpus('/tmp/deerwester.mm')  # comes from the first tutorial, \"From strings to vectors\"\n",
        "# print(corpus)\n",
        "\n",
        "###\n",
        "# from gensim.corpora import Dictionary\n",
        "# from gensim.test.utils import get_tmpfile\n",
        "\n",
        "# tmp_fname = get_tmpfile(\"dictionary\")\n",
        "# corpus = [[\"máma\", \"mele\", \"maso\"], [\"ema\", \"má\", \"máma\"]]\n",
        "\n",
        "\n",
        "# dct = Dictionary(corpus)\n",
        "# dct.save_as_text(tmp_fname)\n",
        "\n",
        "# loaded_dct = Dictionary.load_from_text(tmp_fname)\n",
        "# assert dct.token2id == loaded_dct.token2id\n",
        "###\n",
        "\n",
        "###\n",
        "# from gensim.test.utils import common_dictionary, common_corpus\n",
        "# from gensim.models import LsiModel\n",
        "\n",
        "# model = LsiModel(common_corpus, id2word=common_dictionary)\n",
        "\n",
        "###\n",
        "\n",
        "# vectorized_corpus = lsi[corpus]  # vectorize input copus in BoW format\n",
        "\n",
        "# dictionary = Dictionary(common_corpus)\n",
        "# corpus = common_corpus\n",
        "\n",
        "# lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)\n",
        "\n",
        "\n",
        "# index = similarities.MatrixSimilarity(lsi[corpus])  # transform corpus to LSI space and index it\n",
        "\n",
        "# print('')\n",
        "# print('')\n",
        "# Index persistency is handled via the standard save() and load() functions:\n",
        "# index.save('/tmp/deerwester.index')\n",
        "# index = similarities.MatrixSimilarity.load('/tmp/deerwester.index')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jj245c1VQ9f7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "573b0d8e-70e6-40f3-efd9-000542c1de66"
      },
      "source": [
        "from gensim.summarization import keywords\n",
        "from gensim.summarization.summarizer import summarize\n",
        "\n",
        "docs = [\n",
        "     \"The bar chart is not working\"\n",
        "    ,\"Really like the report! Would be nice to add drill down.\"\n",
        "    ,\"You did a great job!\"\n",
        "    ,\"could use better visuals\"\n",
        "    ,\"needs improvement\"\n",
        "    ,\"a lot of work was put into this report and it came out great, nice work\"\n",
        "    ,\"load times are a little slow\"\n",
        "]\n",
        "\n",
        "for i in range(len(docs)):\n",
        "  key_words = keywords(docs[i],ratio=0.95) #,pos_filter=('NN')\n",
        "#   summary = summarize(i)\n",
        "  print(i, key_words)\n",
        "  \n",
        "  "
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 bar\n",
            "1 add\n",
            "2 great\n",
            "3 visuals\n",
            "4 needs\n",
            "5 nice\n",
            "work\n",
            "6 load times\n",
            "little\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMCF1rqUiy0l",
        "colab_type": "code",
        "outputId": "a49090d8-867b-4f34-e637-c60fc071fae1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "# Gensim Test per https://radimrehurek.com/gensim/tut3.html\n",
        "# https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python\n",
        "\n",
        "# to see logging events\n",
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "\n",
        "\n",
        "import re \n",
        "documents = [\n",
        "    \"Bug fix issue problem broke innaccurate wrong poor slow error working\"\n",
        "    ,\"Great nice perfect awesome good fast excellent love well like\"\n",
        "    ,\"Enhance add new addition future modify update improve augment enrich\"\n",
        "    ]\n",
        "\n",
        "stoplist = set('for a of the and to in'.split())\n",
        "\n",
        "texts = [\n",
        "    [re.sub('[^A-Za-z0-9]+', '', word) for word in document.lower().split() if word not in stoplist]\n",
        "    for document in documents\n",
        "    ]\n",
        "print(texts)\n",
        "from gensim import corpora \n",
        "dictionary = corpora.Dictionary(texts)\n",
        "print(dictionary.token2id)\n",
        "\n",
        "corpus = [dictionary.doc2bow(text) for text in texts]\n",
        "print(corpus)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-08-04 06:14:19,118 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
            "2019-08-04 06:14:19,120 : INFO : built Dictionary(31 unique tokens: ['broke', 'bug', 'error', 'fix', 'innaccurate']...) from 3 documents (total 31 corpus positions)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "great\n",
            "[['bug', 'fix', 'issue', 'problem', 'broke', 'innaccurate', 'wrong', 'poor', 'slow', 'error', 'working'], ['great', 'nice', 'perfect', 'awesome', 'good', 'fast', 'excellent', 'love', 'well', 'like'], ['enhance', 'add', 'new', 'addition', 'future', 'modify', 'update', 'improve', 'augment', 'enrich']]\n",
            "{'broke': 0, 'bug': 1, 'error': 2, 'fix': 3, 'innaccurate': 4, 'issue': 5, 'poor': 6, 'problem': 7, 'slow': 8, 'working': 9, 'wrong': 10, 'awesome': 11, 'excellent': 12, 'fast': 13, 'good': 14, 'great': 15, 'like': 16, 'love': 17, 'nice': 18, 'perfect': 19, 'well': 20, 'add': 21, 'addition': 22, 'augment': 23, 'enhance': 24, 'enrich': 25, 'future': 26, 'improve': 27, 'modify': 28, 'new': 29, 'update': 30}\n",
            "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)], [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1)], [(21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9dar7SKqvRr",
        "colab_type": "code",
        "outputId": "6818c8c3-dd55-480f-f55f-fecbc8727659",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "from gensim import models\n",
        "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=3)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-08-04 06:04:17,687 : INFO : using serial LSI version on this node\n",
            "2019-08-04 06:04:17,689 : INFO : updating model with new documents\n",
            "2019-08-04 06:04:17,699 : INFO : preparing a new chunk of documents\n",
            "2019-08-04 06:04:17,702 : INFO : using 100 extra samples and 2 power iterations\n",
            "2019-08-04 06:04:17,705 : INFO : 1st phase: constructing (31, 103) action matrix\n",
            "2019-08-04 06:04:17,707 : INFO : orthonormalizing (31, 103) action matrix\n",
            "2019-08-04 06:04:17,712 : INFO : 2nd phase: running dense svd on (31, 3) matrix\n",
            "2019-08-04 06:04:17,714 : INFO : computing the final decomposition\n",
            "2019-08-04 06:04:17,715 : INFO : keeping 3 factors (discarding 0.000% of energy spectrum)\n",
            "2019-08-04 06:04:17,716 : INFO : processed documents up to #3\n",
            "2019-08-04 06:04:17,718 : INFO : topic #0(3.317): 0.302*\"poor\" + 0.302*\"problem\" + 0.302*\"wrong\" + 0.302*\"working\" + 0.302*\"slow\" + 0.302*\"bug\" + 0.302*\"innaccurate\" + 0.302*\"error\" + 0.302*\"issue\" + 0.302*\"fix\"\n",
            "2019-08-04 06:04:17,720 : INFO : topic #1(3.162): 0.244*\"awesome\" + 0.244*\"well\" + 0.244*\"like\" + 0.244*\"nice\" + 0.244*\"good\" + 0.244*\"fast\" + 0.244*\"excellent\" + 0.244*\"perfect\" + 0.244*\"great\" + 0.244*\"love\"\n",
            "2019-08-04 06:04:17,721 : INFO : topic #2(3.162): 0.244*\"modify\" + 0.244*\"augment\" + 0.244*\"enrich\" + 0.244*\"new\" + 0.244*\"improve\" + 0.244*\"future\" + 0.244*\"enhance\" + 0.244*\"add\" + 0.244*\"update\" + 0.244*\"addition\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeiXIhcCkn4F",
        "colab_type": "code",
        "outputId": "1ca825dd-8a04-45d7-a79c-d4db86d2b509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "# 0 Bug\n",
        "# 1 Nice\n",
        "# 2 Enhance\n",
        "\n",
        "docs = [\n",
        "     \"The bar chart is not working\"\n",
        "    ,\"Really like the report! Would be nice to add drill down.\"\n",
        "    ,\"You did a great job!\"\n",
        "    ,\"\"\n",
        "    ,\"\"\n",
        "]\n",
        "    \n",
        "doc = re.sub('[^A-Za-z0-9]+', ' ', docs[2]) # select doc\n",
        "print(doc)\n",
        "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
        "vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n",
        "# print(vec_lsi)\n",
        "# print('')\n",
        "# print('')\n",
        "\n",
        "# To prepare for similarity queries, we need to enter all documents which we want to compare against subsequent queries. \n",
        "# In our case, they are the same nine documents used for training LSI, converted to 2-D LSA space. \n",
        "# But that’s only incidental, we might also be indexing a different corpus altogether.\n",
        "\n",
        "import gensim\n",
        "from gensim.similarities import Similarity\n",
        "\n",
        "from gensim.test.utils import get_tmpfile\n",
        "\n",
        "index_tmpfile = get_tmpfile(\"index\")\n",
        "\n",
        "\n",
        "# index = Similarity(index_tmpfile, corpus, num_features=len(dictionary))  # build the index\n",
        "# similarities = index[corpus]  # get similarities between the query and all index documents\n",
        "\n",
        "index = gensim.similarities.MatrixSimilarity(lsi[corpus])\n",
        "\n",
        "# To obtain similarities of our query document against the nine indexed documents:\n",
        "sims = index[vec_lsi]  # perform a similarity query against the corpus\n",
        "# print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples\n",
        "\n",
        "# With some standard Python magic we sort these similarities into descending order, and obtain the final answer \n",
        "# to the query “Human computer interaction”:\n",
        "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
        "print(sims)  # print sorted (document number, similarity score) 2-tuples"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-08-04 06:04:19,231 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
            "2019-08-04 06:04:19,234 : INFO : creating matrix with 3 documents and 3 features\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "You did a great job \n",
            "[(1, 1.0), (2, 2.9802322e-08), (0, 0.0)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7ciG_2osUNT",
        "colab_type": "code",
        "outputId": "08679550-ed96-40fc-9f21-52cf77dfa6f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 0.5), (1, 0.0), (2, 0.0)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/similarities/docsim.py:518: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = numpy.hstack(shard_results)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH4VI6z6xlGs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Gensim Server Test\n",
        "# https://radimrehurek.com/gensim/simserver.html"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU2W5MbqxmxT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import sqlite\n",
        "# import simserver\n",
        "# from simserver import SessionServer"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}